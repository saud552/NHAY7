"""
ğŸ’¾ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø°ÙƒÙŠ
============================

Ù†Ø¸Ø§Ù… Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ÙŠÙ‚Ù„Ù„ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆÙƒÙŠØ²:
- ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
- Ø¶ØºØ· ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
- Ø¥Ø¯Ø§Ø±Ø© Ø°ÙƒÙŠØ© Ù„Ù„Ù…Ø³Ø§Ø­Ø©
- ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
"""

import asyncio
import hashlib
import json
import os
import time
import zlib
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import sqlite3
import aiofiles

from ZeMusic import LOGGER

@dataclass
class CacheEntry:
    key: str
    data: bytes
    created_at: float
    last_accessed: float
    access_count: int
    size: int
    compressed: bool = False
    metadata: Dict[str, Any] = None

class SmartCache:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø°ÙƒÙŠ"""
    
    def __init__(self, cache_dir: str = "smart_cache", max_size_gb: float = 5.0):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        self.max_size_bytes = int(max_size_gb * 1024 * 1024 * 1024)  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø¨Ø§ÙŠØª
        self.compression_threshold = 1024 * 100  # 100KB
        self.max_age_days = 30
        self.cleanup_interval = 3600  # Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
        
        # Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.db_path = self.cache_dir / "cache.db"
        self._init_database()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats = {
            'hits': 0,
            'misses': 0,
            'total_requests': 0,
            'cache_size': 0,
            'entries_count': 0
        }
        
        # Ø¢Ø®Ø± Ø¹Ù…Ù„ÙŠØ© ØªÙ†Ø¸ÙŠÙ
        self.last_cleanup = 0
        
        LOGGER(__name__).info(f"ğŸ’¾ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø°ÙƒÙŠ - Ù…Ø¬Ù„Ø¯: {cache_dir}")
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ù†Ø¯ Ø§Ù„Ø¨Ø¯Ø¡
        asyncio.create_task(self._update_stats())

    def _init_database(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS cache_entries (
                    key TEXT PRIMARY KEY,
                    file_path TEXT,
                    created_at REAL,
                    last_accessed REAL,
                    access_count INTEGER DEFAULT 0,
                    size INTEGER,
                    compressed INTEGER DEFAULT 0,
                    metadata TEXT
                )
            ''')
            
            # Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø§Ø±Ø³ Ù„Ù„Ø£Ø¯Ø§Ø¡
            conn.execute('CREATE INDEX IF NOT EXISTS idx_last_accessed ON cache_entries(last_accessed)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_access_count ON cache_entries(access_count)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_created_at ON cache_entries(created_at)')
            
            conn.commit()

    async def get(self, key: str) -> Optional[bytes]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        self.stats['total_requests'] += 1
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø¯Ø®Ù„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(
                'SELECT file_path, compressed, access_count FROM cache_entries WHERE key = ?', 
                (key,)
            )
            result = cursor.fetchone()
        
        if not result:
            self.stats['misses'] += 1
            return None
        
        file_path, compressed, access_count = result
        cache_file = self.cache_dir / file_path
        
        if not cache_file.exists():
            # Ù…Ù„Ù Ù…Ø­Ø°ÙˆÙØŒ Ø¥Ø²Ø§Ù„Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            await self._remove_entry(key)
            self.stats['misses'] += 1
            return None
        
        try:
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            async with aiofiles.open(cache_file, 'rb') as f:
                data = await f.read()
            
            # Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø¶ØºØ· Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            if compressed:
                data = zlib.decompress(data)
            
            # ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙˆØµÙˆÙ„
            current_time = time.time()
            with sqlite3.connect(self.db_path) as conn:
                conn.execute(
                    'UPDATE cache_entries SET last_accessed = ?, access_count = access_count + 1 WHERE key = ?',
                    (current_time, key)
                )
                conn.commit()
            
            self.stats['hits'] += 1
            LOGGER(__name__).debug(f"ğŸ’¾ cache hit: {key}")
            
            return data
            
        except Exception as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª {key}: {e}")
            await self._remove_entry(key)
            self.stats['misses'] += 1
            return None

    async def set(self, key: str, data: bytes, metadata: Dict[str, Any] = None) -> bool:
        """Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¶Ø±ÙˆØ±Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ
            await self._cleanup_if_needed()
            
            current_time = time.time()
            original_size = len(data)
            
            # Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            compressed = False
            if original_size > self.compression_threshold:
                compressed_data = zlib.compress(data, level=6)
                if len(compressed_data) < original_size * 0.8:  # ØªÙˆÙÙŠØ± 20% Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„
                    data = compressed_data
                    compressed = True
                    LOGGER(__name__).debug(f"ğŸ—œï¸ Ø¶ØºØ· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {original_size} -> {len(data)} bytes")
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù Ø¢Ù…Ù†
            file_name = hashlib.md5(key.encode()).hexdigest() + '.cache'
            cache_file = self.cache_dir / file_name
            
            # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            async with aiofiles.open(cache_file, 'wb') as f:
                await f.write(data)
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            with sqlite3.connect(self.db_path) as conn:
                conn.execute(
                    '''INSERT OR REPLACE INTO cache_entries 
                       (key, file_path, created_at, last_accessed, access_count, size, compressed, metadata)
                       VALUES (?, ?, ?, ?, 1, ?, ?, ?)''',
                    (key, file_name, current_time, current_time, len(data), 
                     int(compressed), json.dumps(metadata or {}))
                )
                conn.commit()
            
            LOGGER(__name__).debug(f"ğŸ’¾ cache set: {key} ({len(data)} bytes)")
            await self._update_stats()
            
            return True
            
        except Exception as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª {key}: {e}")
            return False

    async def exists(self, key: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…ÙØªØ§Ø­ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('SELECT 1 FROM cache_entries WHERE key = ?', (key,))
            return cursor.fetchone() is not None

    async def delete(self, key: str) -> bool:
        """Ø­Ø°Ù Ù…Ø¯Ø®Ù„ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        return await self._remove_entry(key)

    async def _remove_entry(self, key: str) -> bool:
        """Ø¥Ø²Ø§Ù„Ø© Ù…Ø¯Ø®Ù„ Ù…Ø¹ Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù"""
        try:
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute('SELECT file_path FROM cache_entries WHERE key = ?', (key,))
                result = cursor.fetchone()
                
                if result:
                    file_path = result[0]
                    cache_file = self.cache_dir / file_path
                    
                    # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù
                    if cache_file.exists():
                        cache_file.unlink()
                    
                    # Ø­Ø°Ù Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    conn.execute('DELETE FROM cache_entries WHERE key = ?', (key,))
                    conn.commit()
                    
                    return True
            
            return False
            
        except Exception as e:
            LOGGER(__name__).error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ø°Ù Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª {key}: {e}")
            return False

    async def _cleanup_if_needed(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±"""
        current_time = time.time()
        
        if (current_time - self.last_cleanup) < self.cleanup_interval:
            return
        
        await self._cleanup()
        self.last_cleanup = current_time

    async def _cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        LOGGER(__name__).info("ğŸ§¹ Ø¨Ø¯Ø¡ ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª...")
        
        current_time = time.time()
        max_age_seconds = self.max_age_days * 24 * 3600
        
        # Ø­Ø°Ù Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        with sqlite3.connect(self.db_path) as conn:
            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            cursor = conn.execute(
                'SELECT key, file_path FROM cache_entries WHERE ? - created_at > ?',
                (current_time, max_age_seconds)
            )
            old_entries = cursor.fetchall()
            
            # Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            for key, file_path in old_entries:
                cache_file = self.cache_dir / file_path
                if cache_file.exists():
                    cache_file.unlink()
            
            if old_entries:
                conn.execute(
                    'DELETE FROM cache_entries WHERE ? - created_at > ?',
                    (current_time, max_age_seconds)
                )
                LOGGER(__name__).info(f"ğŸ—‘ï¸ ØªÙ… Ø­Ø°Ù {len(old_entries)} Ù…Ø¯Ø®Ù„ Ù‚Ø¯ÙŠÙ…")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        await self._enforce_size_limit()
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        await self._update_stats()
        
        LOGGER(__name__).info("âœ… ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª")

    async def _enforce_size_limit(self):
        """ÙØ±Ø¶ Ø­Ø¯ Ø­Ø¬Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        current_size = await self._calculate_total_size()
        
        if current_size <= self.max_size_bytes:
            return
        
        LOGGER(__name__).info(f"ğŸ“¦ Ø­Ø¬Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯: {current_size / 1024 / 1024:.1f} MB")
        
        # Ø­Ø°Ù Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ø£Ù‚Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹
        with sqlite3.connect(self.db_path) as conn:
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø¢Ø®Ø± ÙˆØµÙˆÙ„ Ø«Ù… Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØµÙˆÙ„
            cursor = conn.execute(
                '''SELECT key, file_path, size FROM cache_entries 
                   ORDER BY last_accessed ASC, access_count ASC'''
            )
            entries = cursor.fetchall()
            
            deleted_size = 0
            target_size = self.max_size_bytes * 0.8  # Ø­Ø°Ù Ø­ØªÙ‰ 80% Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰
            
            for key, file_path, size in entries:
                if current_size - deleted_size <= target_size:
                    break
                
                # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù ÙˆØ§Ù„Ù…Ø¯Ø®Ù„
                cache_file = self.cache_dir / file_path
                if cache_file.exists():
                    cache_file.unlink()
                
                conn.execute('DELETE FROM cache_entries WHERE key = ?', (key,))
                deleted_size += size
            
            conn.commit()
            
            if deleted_size > 0:
                LOGGER(__name__).info(f"ğŸ—‘ï¸ ØªÙ… Ø­Ø°Ù {deleted_size / 1024 / 1024:.1f} MB Ù„ÙØ±Ø¶ Ø­Ø¯ Ø§Ù„Ø­Ø¬Ù…")

    async def _calculate_total_size(self) -> int:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('SELECT SUM(size) FROM cache_entries')
            result = cursor.fetchone()
            return result[0] or 0

    async def _update_stats(self):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('SELECT COUNT(*), SUM(size) FROM cache_entries')
            count, total_size = cursor.fetchone()
            
            self.stats['entries_count'] = count or 0
            self.stats['cache_size'] = total_size or 0

    def get_stats(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        hit_rate = (self.stats['hits'] / max(1, self.stats['total_requests'])) * 100
        
        return {
            'hits': self.stats['hits'],
            'misses': self.stats['misses'],
            'total_requests': self.stats['total_requests'],
            'hit_rate_percent': round(hit_rate, 2),
            'cache_size_mb': round(self.stats['cache_size'] / 1024 / 1024, 2),
            'entries_count': self.stats['entries_count'],
            'max_size_mb': round(self.max_size_bytes / 1024 / 1024, 2)
        }

    async def get_popular_keys(self, limit: int = 10) -> List[Dict[str, Any]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø£ÙƒØ«Ø± Ø´Ø¹Ø¨ÙŠØ©"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(
                '''SELECT key, access_count, size, metadata FROM cache_entries 
                   ORDER BY access_count DESC LIMIT ?''', 
                (limit,)
            )
            
            results = []
            for key, access_count, size, metadata_str in cursor.fetchall():
                try:
                    metadata = json.loads(metadata_str) if metadata_str else {}
                except:
                    metadata = {}
                
                results.append({
                    'key': key,
                    'access_count': access_count,
                    'size_kb': round(size / 1024, 2),
                    'metadata': metadata
                })
            
            return results

    async def preload_popular_content(self):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ø³Ø¨Ù‚ Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø´Ø§Ø¦Ø¹"""
        # Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø´Ø§Ø¦Ø¹ Ù…Ø³Ø¨Ù‚Ø§Ù‹
        # Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
        popular_keys = await self.get_popular_keys(50)
        
        LOGGER(__name__).info(f"ğŸ“ˆ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(popular_keys)} Ù…ÙØªØ§Ø­ Ø´Ø§Ø¦Ø¹ Ù„Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³Ø¨Ù‚")
        
        # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù…Ù†Ø·Ù‚ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³Ø¨Ù‚ Ù‡Ù†Ø§
        return popular_keys


# Ù†Ø³Ø®Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
smart_cache = SmartCache()